{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.5\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pygame\n",
    "DISPLAY = True\n",
    "if not DISPLAY:\n",
    "    os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Dueling Deep Q Network Learning with Priortized Experienced Reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append('game/')\n",
    "import flappy_wrapped as game\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KERNEL = np.array([[-1,-1,-1], [-1, 9,-1],[-1,-1,-1]])\n",
    "def processFrame(frame):\n",
    "    frame = frame[55:288,0:400] #crop image\n",
    "    frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) #convert image to black and white\n",
    "    frame = cv2.resize(frame,(84,84),interpolation=cv2.INTER_AREA)\n",
    "    _ , frame = cv2.threshold(frame,50,255,cv2.THRESH_BINARY)\n",
    "    #frame = cv2.blur(frame,(5,5))\n",
    "    frame = cv2.filter2D(frame,-1,KERNEL)\n",
    "    #frame = cv2.Canny(frame,100,200)\n",
    "    frame = frame.astype(np.float64)/255.0\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#Dueling DQN\n",
    "class DDQN(nn.Module):\n",
    "    def __init__(self,input_shape,nactions):\n",
    "        super(DDQN,self).__init__()\n",
    "        self.nactions = nactions\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0],32,kernel_size=4,stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64,kernel_size=3,stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64,kernel_size=2,stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.fca = nn.Sequential(\n",
    "            nn.Linear( conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear( 256, nactions )\n",
    "        )\n",
    "        \n",
    "        self.fcv = nn.Sequential(\n",
    "            nn.Linear(conv_out_size,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,1)\n",
    "        )\n",
    "        \n",
    "    def _get_conv_out(self,shape):\n",
    "        o = self.conv( torch.zeros(1,*shape) )\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        action_v = self.fca(conv_out)\n",
    "        #value_v = self.fcv(conv_out).expand(x.size(0), self.nactions)\n",
    "        return action_v #value_v + action_v - action_v.mean(1).unsqueeze(1).expand(x.size(0), self.nactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ACTIONS = [0,1]\n",
    "EXPERIENCE_BUFFER_SIZE = 1500\n",
    "STATE_DIM = 4\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1\n",
    "EPSILON_FINAL = 0.01\n",
    "EPSILON_DECAY_FRAMES = (10**3)\n",
    "MEAN_GOAL_REWARD = 20\n",
    "BATCH_SIZE = 32\n",
    "MIN_EXP_BUFFER_SIZE = 1000\n",
    "SYNC_TARGET_FRAMES = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "SKIP_FRAME = 2\n",
    "INITIAL_SKIP = [0,1,0,1,0,1,0,0,0,1,0,1,0,1,0,1,0,0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "class ExperienceBuffer():\n",
    "    def __init__(self,capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        self.priority = collections.deque(maxlen=capacity)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.priority.clear()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self,exp,p):\n",
    "        self.buffer.append(exp)\n",
    "        self.priority.append(p)\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        probs = np.array(self.priority)/sum(np.array(self.priority))\n",
    "        indices = np.random.choice( range(len(self.buffer)), batch_size, p = probs)\n",
    "        states,actions,rewards,dones,next_states = zip(*[ self.buffer[idx] for idx in indices ])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32),\\\n",
    "    np.array(dones,dtype=np.uint8), np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,env,buffer,state_buffer_size = STATE_DIM):\n",
    "        self.env = env\n",
    "        self.exp_buffer = buffer\n",
    "        self.state = collections.deque(maxlen = STATE_DIM)\n",
    "        self.next_state= collections.deque(maxlen = STATE_DIM)\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self):\n",
    "        self.total_rewards = 0\n",
    "        \n",
    "        for i in INITIAL_SKIP[:-1]:\n",
    "            frame,reward,done = self.env.frame_step(i)\n",
    "            self.total_rewards+=reward\n",
    "            if done:\n",
    "                self._reset()\n",
    "        frame,reward,done =  self.env.frame_step(INITIAL_SKIP[-1])\n",
    "        \n",
    "        if done:\n",
    "            self._reset()\n",
    "        \n",
    "        frame = processFrame(frame)\n",
    "        self.state.append(frame)\n",
    "        self.next_state.append(frame)\n",
    "    \n",
    "    def step(self,net,tgt_net,epsilon=0.9,device='cpu'):\n",
    "        done_reward  = None\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(ACTIONS)\n",
    "        else:\n",
    "            state_v = torch.tensor(np.array([self.state],copy=False),dtype=torch.float32).to(device)\n",
    "            action = int(torch.argmax(net(state_v)))\n",
    "       \n",
    "        frame,reward,done = self.env.frame_step(action)\n",
    "        for _ in range(SKIP_FRAME):\n",
    "                frame,reward,done =  self.env.frame_step(action)\n",
    "                self.total_rewards += reward\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "        frame = processFrame(frame)\n",
    "        self.next_state.append(frame)\n",
    "        \n",
    "        if len(self.next_state)==STATE_DIM and len(self.state)==STATE_DIM:\n",
    "            #PER - Prioritized Experience Replay\n",
    "            o = net( torch.tensor( np.array([self.state]),dtype=torch.float32).to(device)).to('cpu').detach().numpy()[0][action]\n",
    "            e = float(torch.max(tgt_net( torch.tensor( np.array([self.next_state]),dtype=torch.float32).to(device))))\n",
    "            p = abs(o-e)+0.0001\n",
    "            self.exp_buffer.append((self.state.copy(),action,int(self.total_rewards),done,self.next_state.copy()),p)\n",
    "        \n",
    "        self.state.append(frame)\n",
    "        if done:\n",
    "            done_reward = self.total_rewards\n",
    "            self._reset()\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_loss(batch,net,tgt_net,device='cpu'):\n",
    "    states,actions,rewards,dones,next_states = batch\n",
    "    \n",
    "    states_v = torch.tensor(states,dtype=torch.float32).to(device)\n",
    "    actions_v = torch.tensor(actions,dtype=torch.long).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    dones_v = torch.ByteTensor(dones).to(device)\n",
    "    next_states_v = torch.tensor(next_states,dtype=torch.float32).to(device)\n",
    "    \n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_action_values = tgt_net(next_states_v).max(1)[0]\n",
    "    next_state_action_values[dones_v] = 0.0\n",
    "    next_state_action_values = next_state_action_values.detach() \n",
    "    \n",
    "    expected_values = rewards_v +  next_state_action_values * GAMMA\n",
    "    return nn.MSELoss()(state_action_values,expected_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "\n",
    "#Double Dueling DQN\n",
    "net = DDQN( (STATE_DIM,84,84), len(ACTIONS) ).to(device)\n",
    "tgt_net = DDQN( (STATE_DIM,84,84), len(ACTIONS) ).to(device)\n",
    "\n",
    "env = game.GameState()\n",
    "buffer = ExperienceBuffer(EXPERIENCE_BUFFER_SIZE)\n",
    "agent = Agent(env,buffer)\n",
    "epsilon = EPSILON_START\n",
    "optimizer = optim.Adam(net.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "total_rewards = []\n",
    "best_mean_reward = float('-inf')\n",
    "last_mean = float('-inf')\n",
    "game_id = 0\n",
    "while True:\n",
    "    epsilon = max( EPSILON_FINAL , EPSILON_START - game_id/EPSILON_DECAY_FRAMES )\n",
    "    \n",
    "    reward = agent.step(net,tgt_net,epsilon,device=device)\n",
    "    if reward is not None:\n",
    "        game_id += 1\n",
    "        total_rewards.append(reward)\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "        print(\"GAME : {} | EPSILON : {:.4f} | MEAN REWARD : {}\".format( game_id, epsilon, mean_reward ))\n",
    "        if best_mean_reward < mean_reward:\n",
    "            best_mean_reward = mean_reward\n",
    "            \n",
    "            if best_mean_reward-last_mean >= 0.2:\n",
    "                torch.save(net.state_dict(),'checkpoints/flappy_best_model.dat')\n",
    "                print(\"REWARD {} -> {}. Model Saved\".format(last_mean,mean_reward))\n",
    "                last_mean = best_mean_reward\n",
    "\n",
    "        \n",
    "        if game_id % SYNC_TARGET_FRAMES == 0:\n",
    "            tgt_net.load_state_dict(net.state_dict())\n",
    "            \n",
    "        if mean_reward >= MEAN_GOAL_REWARD:\n",
    "            print(\"Learned in {} Games.\".format(game_id))\n",
    "            break\n",
    "    \n",
    "    if len(buffer) < MIN_EXP_BUFFER_SIZE:\n",
    "        continue\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch,net,tgt_net,device=device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
